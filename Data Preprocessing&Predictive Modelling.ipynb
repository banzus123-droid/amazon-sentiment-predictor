{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c771e28-6897-41d2-81b2-26d1660520b2",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd63d5a4-464d-4e39-aff1-8b7bf1fefe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\banzu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\banzu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\banzu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\banzu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Dataset Shape: (80291, 8)\n",
      "                               reviewId          userName  \\\n",
      "0  ed55cacc-b2da-4626-9a71-4299c5824726        Tene Smith   \n",
      "1  ffda6f08-60df-4c45-8c13-82cef4959f5d            Fucc U   \n",
      "2  3d416dda-8de9-4bfd-9f2d-38c20a010266  Derek and Sarita   \n",
      "\n",
      "                                             content  score  thumbsUpCount  \\\n",
      "0                         Great shopping experience.      5            0.0   \n",
      "1  I love Amazon but I'm tired of being punished ...      1            0.0   \n",
      "2  Shopping couldn't be easier and should you nee...      5            0.0   \n",
      "\n",
      "  reviewCreatedVersion               at   appVersion  \n",
      "0          30.20.0.100  2025-11-1 11:27  30.20.0.100  \n",
      "1          30.19.0.100  2025-11-1 10:42  30.19.0.100  \n",
      "2          30.20.0.100   2025-11-1 9:54  30.20.0.100  \n",
      "\n",
      "--- BEFORE MISSING VALUES ---\n",
      "content           6\n",
      "score             0\n",
      "thumbsUpCount    24\n",
      "dtype: int64\n",
      "\n",
      "--- AFTER MISSING VALUES ---\n",
      "content          0\n",
      "score            0\n",
      "thumbsUpCount    0\n",
      "dtype: int64\n",
      "\n",
      "--- BEFORE TEXT CLEANING ---\n",
      "0                           Great shopping experience.\n",
      "1    I love Amazon but I'm tired of being punished ...\n",
      "2    Shopping couldn't be easier and should you nee...\n",
      "Name: content, dtype: object\n",
      "\n",
      "--- AFTER TEXT CLEANING ---\n",
      "0                            great shopping experience\n",
      "1    love amazon im tired punished bc wont pay mont...\n",
      "2    shopping couldnt easier need return anything c...\n",
      "Name: clean_content, dtype: object\n",
      "\n",
      "--- REVIEW LENGTH ---\n",
      "0     3\n",
      "1    43\n",
      "2    12\n",
      "3     8\n",
      "4    11\n",
      "Name: review_length, dtype: int64\n",
      "\n",
      "--- BEFORE OUTLIER CLIPPING ---\n",
      "       review_length  thumbsUpCount\n",
      "count   80285.000000   80285.000000\n",
      "mean       18.142306       9.635411\n",
      "std        13.724363      75.124098\n",
      "min         0.000000       0.000000\n",
      "25%         8.000000       0.000000\n",
      "50%        15.000000       0.000000\n",
      "75%        26.000000       1.000000\n",
      "max       144.000000    5660.000000\n",
      "\n",
      "--- AFTER OUTLIER CLIPPING ---\n",
      "       review_length  thumbsUpCount\n",
      "count   80285.000000   80285.000000\n",
      "mean       18.099035       7.016192\n",
      "std        13.588613      29.885001\n",
      "min         0.000000       0.000000\n",
      "25%         8.000000       0.000000\n",
      "50%        15.000000       0.000000\n",
      "75%        26.000000       1.000000\n",
      "max        51.000000     224.000000\n",
      "\n",
      "--- BEFORE LOG TRANSFORMATION ---\n",
      "   review_length  thumbsUpCount\n",
      "0              3            0.0\n",
      "1             43            0.0\n",
      "2             12            0.0\n",
      "3              8            0.0\n",
      "4             11            0.0\n",
      "       review_length  thumbsUpCount\n",
      "count   80285.000000   80285.000000\n",
      "mean       18.099035       7.016192\n",
      "std        13.588613      29.885001\n",
      "min         0.000000       0.000000\n",
      "25%         8.000000       0.000000\n",
      "50%        15.000000       0.000000\n",
      "75%        26.000000       1.000000\n",
      "max        51.000000     224.000000\n",
      "\n",
      "--- AFTER LOG TRANSFORMATION ---\n",
      "   review_length  thumbsUpCount\n",
      "0       1.386294            0.0\n",
      "1       3.784190            0.0\n",
      "2       2.564949            0.0\n",
      "3       2.197225            0.0\n",
      "4       2.484907            0.0\n",
      "\n",
      "--- BEFORE NORMALISATION ---\n",
      "       review_length  thumbsUpCount\n",
      "count   80285.000000   80285.000000\n",
      "mean        2.632269       0.659862\n",
      "std         0.886419       1.147575\n",
      "min         0.000000       0.000000\n",
      "25%         2.197225       0.000000\n",
      "50%         2.772589       0.000000\n",
      "75%         3.295837       0.693147\n",
      "max         3.951244       5.416100\n",
      "\n",
      "--- AFTER NORMALISATION ---\n",
      "       review_length  thumbsUpCount\n",
      "count   8.028500e+04   8.028500e+04\n",
      "mean   -8.213037e-17   1.104512e-16\n",
      "std     1.000006e+00   1.000006e+00\n",
      "min    -2.969572e+00  -5.750092e-01\n",
      "25%    -4.907920e-01  -5.750092e-01\n",
      "50%     1.583002e-01  -5.750092e-01\n",
      "75%     7.485982e-01   2.900510e-02\n",
      "max     1.487990e+00   4.144626e+00\n",
      "\n",
      "--- BEFORE TARGET ENCODING ---\n",
      "0    5\n",
      "1    1\n",
      "2    5\n",
      "3    3\n",
      "4    4\n",
      "Name: score, dtype: int64\n",
      "\n",
      "--- AFTER TARGET ENCODING ---\n",
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "--- TF-IDF SHAPE --- (80285, 36258)\n",
      "--- SELECTED FEATURES SHAPE --- (80285, 5000)\n",
      "Before Feature Selection: (80285, 36258)\n",
      "After Feature Selection: (80285, 5000)\n",
      "\n",
      "--- FINAL DATASET ---\n",
      "X shape: (80285, 5002)\n",
      "y shape: (80285,)\n",
      "Saved: amazon_orange_readable.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ================= LOAD DATA =================\n",
    "df = pd.read_csv(\"amazon_review_dataset.csv\")\n",
    "print(\"Initial Dataset Shape:\", df.shape)\n",
    "print(df.head(3))\n",
    "\n",
    "# ================= MISSING VALUES =================\n",
    "print(\"\\n--- BEFORE MISSING VALUES ---\")\n",
    "print(df[[\"content\",\"score\",\"thumbsUpCount\"]].isnull().sum())\n",
    "\n",
    "df = df.dropna(subset=[\"content\",\"score\"])\n",
    "df[\"thumbsUpCount\"] = df[\"thumbsUpCount\"].fillna(0)\n",
    "\n",
    "print(\"\\n--- AFTER MISSING VALUES ---\")\n",
    "print(df[[\"content\",\"score\",\"thumbsUpCount\"]].isnull().sum())\n",
    "\n",
    "# ================= TEXT NORMALISATION =================\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"\\n--- BEFORE TEXT CLEANING ---\")\n",
    "print(df[\"content\"].head(3))\n",
    "\n",
    "df[\"clean_content\"] = df[\"content\"].apply(clean_text)\n",
    "\n",
    "print(\"\\n--- AFTER TEXT CLEANING ---\")\n",
    "print(df[\"clean_content\"].head(3))\n",
    "\n",
    "# ================= FEATURE ENGINEERING =================\n",
    "df[\"review_length\"] = df[\"clean_content\"].apply(lambda x: len(x.split()))\n",
    "print(\"\\n--- REVIEW LENGTH ---\")\n",
    "print(df[\"review_length\"].head())\n",
    "\n",
    "# ================= OUTLIER HANDLING =================\n",
    "print(\"\\n--- BEFORE OUTLIER CLIPPING ---\")\n",
    "print(df[[\"review_length\",\"thumbsUpCount\"]].describe())\n",
    "\n",
    "df[\"review_length\"] = df[\"review_length\"].clip(upper=df[\"review_length\"].quantile(0.99))\n",
    "df[\"thumbsUpCount\"] = df[\"thumbsUpCount\"].clip(upper=df[\"thumbsUpCount\"].quantile(0.99))\n",
    "\n",
    "print(\"\\n--- AFTER OUTLIER CLIPPING ---\")\n",
    "print(df[[\"review_length\",\"thumbsUpCount\"]].describe())\n",
    "\n",
    "# ================= SKEWNESS HANDLING =================\n",
    "print(\"\\n--- BEFORE LOG TRANSFORMATION ---\")\n",
    "print(df[[\"review_length\", \"thumbsUpCount\"]].head())\n",
    "print(df[[\"review_length\", \"thumbsUpCount\"]].describe())\n",
    "\n",
    "df[\"review_length\"] = np.log1p(df[\"review_length\"])\n",
    "df[\"thumbsUpCount\"] = np.log1p(df[\"thumbsUpCount\"])\n",
    "print(\"\\n--- AFTER LOG TRANSFORMATION ---\")\n",
    "print(df[[\"review_length\",\"thumbsUpCount\"]].head())\n",
    "\n",
    "# ================= NORMALISATION =================\n",
    "print(\"\\n--- BEFORE NORMALISATION ---\")\n",
    "print(df[[\"review_length\",\"thumbsUpCount\"]].describe())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[[\"thumbsUpCount\",\"review_length\"]] = scaler.fit_transform(df[[\"thumbsUpCount\",\"review_length\"]])\n",
    "\n",
    "print(\"\\n--- AFTER NORMALISATION ---\")\n",
    "print(df[[\"review_length\",\"thumbsUpCount\"]].describe())\n",
    "\n",
    "# ================= TARGET ENCODING =================\n",
    "print(\"\\n--- BEFORE TARGET ENCODING ---\")\n",
    "print(df[\"score\"].head())\n",
    "\n",
    "df[\"sentiment\"] = df[\"score\"].apply(lambda x: 1 if x > 3 else 0)\n",
    "df = df.drop(columns=[\"score\"])\n",
    "\n",
    "print(\"\\n--- AFTER TARGET ENCODING ---\")\n",
    "print(df[\"sentiment\"].head())\n",
    "\n",
    "# ================= FEATURE SELECTION =================\n",
    "X_text = df[\"clean_content\"]\n",
    "X_num = df[[\"thumbsUpCount\",\"review_length\"]]\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=50000)\n",
    "X_tfidf = tfidf.fit_transform(X_text)\n",
    "print(\"\\n--- TF-IDF SHAPE ---\", X_tfidf.shape)\n",
    "\n",
    "selector = SelectKBest(chi2, k=5000)\n",
    "X_selected = selector.fit_transform(X_tfidf, y)\n",
    "print(\"--- SELECTED FEATURES SHAPE ---\", X_selected.shape)\n",
    "\n",
    "X_final = hstack([X_selected, X_num])\n",
    "\n",
    "print(\"Before Feature Selection:\", X_tfidf.shape)\n",
    "print(\"After Feature Selection:\", X_selected.shape)\n",
    "\n",
    "print(\"\\n--- FINAL DATASET ---\")\n",
    "print(\"X shape:\", X_final.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "\n",
    "# ================= Preprocessed File =================\n",
    "\n",
    "orange_df = df[[\n",
    "    \"content\",\n",
    "    \"thumbsUpCount\",\n",
    "    \"review_length\",\n",
    "    \"sentiment\"\n",
    "]]\n",
    "\n",
    "orange_df.to_csv(\"amazon_orange_readable.csv\", index=False)\n",
    "print(\"Saved: amazon_orange_readable.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20223966-ae5f-4055-8f8d-99c90800de85",
   "metadata": {},
   "source": [
    "Trainâ€“Test Split Strategy & Predictive Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b040a2-1ffe-4301-99d0-9e4c36db26f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ SPLIT 60:40 ================\n",
      "Train size: 48171  | Test size: 32114\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.9001681509621972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93     20823\n",
      "           1       0.90      0.80      0.85     11291\n",
      "\n",
      "    accuracy                           0.90     32114\n",
      "   macro avg       0.90      0.88      0.89     32114\n",
      "weighted avg       0.90      0.90      0.90     32114\n",
      "\n",
      "\n",
      "Model: SVM\n",
      "Accuracy: 0.9013514355109921\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93     20823\n",
      "           1       0.90      0.81      0.85     11291\n",
      "\n",
      "    accuracy                           0.90     32114\n",
      "   macro avg       0.90      0.88      0.89     32114\n",
      "weighted avg       0.90      0.90      0.90     32114\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.8815158497851404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     20823\n",
      "           1       0.92      0.73      0.81     11291\n",
      "\n",
      "    accuracy                           0.88     32114\n",
      "   macro avg       0.89      0.85      0.86     32114\n",
      "weighted avg       0.88      0.88      0.88     32114\n",
      "\n",
      "\n",
      "Model: kNN\n",
      "Accuracy: 0.7755496045338481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84     20823\n",
      "           1       0.74      0.56      0.64     11291\n",
      "\n",
      "    accuracy                           0.78     32114\n",
      "   macro avg       0.76      0.73      0.74     32114\n",
      "weighted avg       0.77      0.78      0.77     32114\n",
      "\n",
      "\n",
      "================ SPLIT 70:30 ================\n",
      "Train size: 56199  | Test size: 24086\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.9021838412355725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     15618\n",
      "           1       0.91      0.80      0.85      8468\n",
      "\n",
      "    accuracy                           0.90     24086\n",
      "   macro avg       0.90      0.88      0.89     24086\n",
      "weighted avg       0.90      0.90      0.90     24086\n",
      "\n",
      "\n",
      "Model: SVM\n",
      "Accuracy: 0.9031387528024578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93     15618\n",
      "           1       0.90      0.81      0.85      8468\n",
      "\n",
      "    accuracy                           0.90     24086\n",
      "   macro avg       0.90      0.88      0.89     24086\n",
      "weighted avg       0.90      0.90      0.90     24086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# ================= SPLIT RATIOS =================\n",
    "split_ratios = {\n",
    "    \"60:40\": 0.40,\n",
    "    \"70:30\": 0.30,\n",
    "    \"80:20\": 0.20,\n",
    "    \"90:10\": 0.10\n",
    "}\n",
    "\n",
    "# ================= MODELS =================\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"SVM\": LinearSVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"kNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# ================= TRAIN & EVALUATE =================\n",
    "for split_name, test_size in split_ratios.items():\n",
    "    print(f\"\\n================ SPLIT {split_name} ================\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_final, y, test_size=test_size, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"Train size:\", len(y_train), \" | Test size:\", len(y_test))\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        print(\"Accuracy:\", acc)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ================= SAVE MODEL FOR APP =================\n",
    "# Using Logistic Regression with 80:20 split as it's simple and performs well\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model and preprocessing objects\n",
    "joblib.dump(model, 'sentiment_model.pkl')\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(selector, 'feature_selector.pkl')\n",
    "\n",
    "print(\"\\nModel and preprocessing objects saved for the app.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
